{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "C function cuda.ccudart.cudaStreamSynchronize has wrong signature (expected __pyx_t_4cuda_7ccudart_cudaError_t (__pyx_t_4cuda_7ccudart_cudaStream_t), got cudaError_t (cudaStream_t))",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# LOAD LIBRARIES\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\u001B[38;5;241m,\u001B[39m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m \u001B[38;5;66;03m# CPU libraries\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcupy\u001B[39;00m\u001B[38;5;241m,\u001B[39m \u001B[38;5;21;01mcudf\u001B[39;00m \u001B[38;5;66;03m# GPU libraries\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\u001B[38;5;241m,\u001B[39m \u001B[38;5;21;01mgc\u001B[39;00m\u001B[38;5;241m,\u001B[39m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRAPIDS version\u001B[39m\u001B[38;5;124m'\u001B[39m,cudf\u001B[38;5;241m.\u001B[39m__version__)\n",
      "File \u001B[0;32m~/anaconda3/envs/rapids-22.06/lib/python3.9/site-packages/cudf/__init__.py:10\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcupy\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumba\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config \u001B[38;5;28;01mas\u001B[39;00m numba_config, cuda\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrmm\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcudf\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtypes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dtype\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcudf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m api, core, datasets, testing\n",
      "File \u001B[0;32m~/anaconda3/envs/rapids-22.06/lib/python3.9/site-packages/rmm/__init__.py:17\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mweakref\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mrmm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mr\n\u001B[0;32m---> 17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mrmm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_lib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdevice_buffer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DeviceBuffer\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mrmm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_version\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_versions\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mrmm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmr\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m disable_logging, enable_logging, get_log_filenames\n",
      "File \u001B[0;32m~/anaconda3/envs/rapids-22.06/lib/python3.9/site-packages/rmm/_lib/__init__.py:15\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright (c) 2019-2021, NVIDIA CORPORATION.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# See the License for the specific language governing permissions and\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdevice_buffer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DeviceBuffer\n",
      "File \u001B[0;32mdevice_buffer.pyx:1\u001B[0m, in \u001B[0;36minit rmm._lib.device_buffer\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: C function cuda.ccudart.cudaStreamSynchronize has wrong signature (expected __pyx_t_4cuda_7ccudart_cudaError_t (__pyx_t_4cuda_7ccudart_cudaStream_t), got cudaError_t (cudaStream_t))"
     ]
    }
   ],
   "source": [
    "# LOAD LIBRARIES\n",
    "import pandas as pd, numpy as np # CPU libraries\n",
    "import cupy, cudf # GPU libraries\n",
    "import matplotlib.pyplot as plt, gc, os\n",
    "print('RAPIDS version',cudf.__version__)"
   ]
  },
  {
   "cell_type": "raw",
   "source": [
    "conda create -n rapids-22.06 -c rapidsai -c nvidia -c conda-forge  \\\n",
    "    cudf=22.06 python=3.9 cudatoolkit=11.7"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# version name for saved model files\n",
    "VER = 1\n",
    "\n",
    "# train random seed\n",
    "SEED = 42\n",
    "\n",
    "# fill nan value\n",
    "NAN_VALUE = -127 # will fit in int8\n",
    "\n",
    "# folds per model\n",
    "FOLDS = 5\n",
    "\n",
    "# data directory\n",
    "data = 'data'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def read_file(path = '', usecols = None):\n",
    "    # load dataframe\n",
    "    if usecols is not None:\n",
    "        df = cudf.read_parquet(path, columns=usecols)\n",
    "    else:\n",
    "        df = cudf.read_parquet(path)\n",
    "    # reduce dtype for customer and date\n",
    "    df['customer_ID'] = df['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n",
    "    df.S_2 = cudf.to_datetime( df.S_2 )\n",
    "    # SORT BY CUSTOMER AND DATE (so agg('last') works correctly)\n",
    "    # df = df.sort_values(['customer_ID','S_2'])\n",
    "    # df = df.reset_index(drop=True)\n",
    "    # FILL NAN\n",
    "    df = df.fillna(NAN_VALUE)\n",
    "    print('shape of data:', df.shape)\n",
    "\n",
    "    return df\n",
    "\n",
    "print('Reading train data...')\n",
    "TRAIN_PATH = os.path.join(data, 'train.parquet')\n",
    "train = read_file(path = TRAIN_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_and_feature_engineer(df):\n",
    "    # FEATURE ENGINEERING FROM\n",
    "    # https://www.kaggle.com/code/huseyincot/amex-agg-data-how-it-created\n",
    "    all_cols = [c for c in list(df.columns) if c not in ['customer_ID','S_2']]\n",
    "    cat_features = [\"B_30\",\"B_38\",\"D_114\",\"D_116\",\"D_117\",\"D_120\",\"D_126\",\"D_63\",\"D_64\",\"D_66\",\"D_68\"]\n",
    "    num_features = [col for col in all_cols if col not in cat_features]\n",
    "\n",
    "    test_num_agg = df.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
    "\n",
    "    test_cat_agg = df.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
    "    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
    "\n",
    "    df = cudf.concat([test_num_agg, test_cat_agg], axis=1)\n",
    "    del test_num_agg, test_cat_agg\n",
    "    print('shape after engineering', df.shape )\n",
    "\n",
    "    return df\n",
    "\n",
    "train = process_and_feature_engineer(train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# add targets\n",
    "targets = cudf.read_csv(os.path.join(data, 'train_labels.csv'))\n",
    "targets['customer_ID'] = targets['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n",
    "targets = targets.set_index('customer_ID')\n",
    "train = train.merge(targets, left_index=True, right_index=True, how='left')\n",
    "train.target = train.target.astype('int8')\n",
    "del targets\n",
    "\n",
    "# NEEDED TO MAKE CV DETERMINISTIC (cudf merge above randomly shuffles rows)\n",
    "train = train.sort_index().reset_index()\n",
    "\n",
    "# FEATURES\n",
    "FEATURES = train.columns[1:-1]\n",
    "print(f'There are {len(FEATURES)} features!')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# LOAD XGB LIBRARY\n",
    "from sklearn.model_selection import KFold\n",
    "import xgboost as xgb\n",
    "print('XGB Version',xgb.__version__)\n",
    "\n",
    "# xgb model parameters\n",
    "xgb_parms = {\n",
    "    'max_depth': 4,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'eval_metric': 'logloss',\n",
    "    'objective': 'binary:logistic',\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'predictor': 'gpu_predictor',\n",
    "    'random_state': SEED\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# needed with DeviceQuantileDMatrix below\n",
    "class IterLoadForDMatrix(xgb.core.DataIter):\n",
    "    def __init__(self, df=None, features=None, target=None, batch_size=256*1024):\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.df = df\n",
    "        self.it = 0 # set iterator to 0\n",
    "        self.batch_size = batch_size\n",
    "        self.batches = int( np.ceil( len(df) / self.batch_size ) )\n",
    "        super().__init__()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the iterator\"\"\"\n",
    "        self.it = 0\n",
    "\n",
    "    def next(self, input_data):\n",
    "        \"\"\"Yield next batch of data.\"\"\"\n",
    "        if self.it == self.batches:\n",
    "            return 0 # Return 0 when there's no more batch.\n",
    "\n",
    "        a = self.it * self.batch_size\n",
    "        b = min( (self.it + 1) * self.batch_size, len(self.df) )\n",
    "        dt = cudf.DataFrame(self.df.iloc[a:b])\n",
    "        input_data(data=dt[self.features], label=dt[self.target]) #, weight=dt['weight'])\n",
    "        self.it += 1\n",
    "        return 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/kyakovlev\n",
    "# https://www.kaggle.com/competitions/amex-default-prediction/discussion/327534\n",
    "def amex_metric_mod(y_true, y_pred):\n",
    "\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "importances = []\n",
    "oof = []\n",
    "train = train.to_pandas() # free GPU memory\n",
    "TRAIN_SUBSAMPLE = 1.0\n",
    "gc.collect()\n",
    "\n",
    "skf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
    "for fold,(train_idx, valid_idx) in enumerate(skf.split(\n",
    "            train, train.target )):\n",
    "\n",
    "    # train with subsample of train fold data\n",
    "    if TRAIN_SUBSAMPLE<1.0:\n",
    "        np.random.seed(SEED)\n",
    "        train_idx = np.random.choice(train_idx,\n",
    "                       int(len(train_idx)*TRAIN_SUBSAMPLE), replace=False)\n",
    "        np.random.seed(None)\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### Fold',fold+1)\n",
    "    print('### Train size',len(train_idx),'Valid size',len(valid_idx))\n",
    "    print(f'### Training with {int(TRAIN_SUBSAMPLE*100)}% fold data...')\n",
    "    print('#'*25)\n",
    "\n",
    "    # train, valid, test for fold k\n",
    "    Xy_train = IterLoadForDMatrix(train.loc[train_idx], FEATURES, 'target')\n",
    "    X_valid = train.loc[valid_idx, FEATURES]\n",
    "    y_valid = train.loc[valid_idx, 'target']\n",
    "\n",
    "    dtrain = xgb.DeviceQuantileDMatrix(Xy_train, max_bin=256)\n",
    "    dvalid = xgb.DMatrix(data=X_valid, label=y_valid)\n",
    "\n",
    "    # train model fold k\n",
    "    model = xgb.train(xgb_parms,\n",
    "                dtrain=dtrain,\n",
    "                evals=[(dtrain,'train'),(dvalid,'valid')],\n",
    "                num_boost_round=9999,\n",
    "                early_stopping_rounds=100,\n",
    "                verbose_eval=100)\n",
    "    model.save_model(f'XGB_v{VER}_fold{fold}.xgb')\n",
    "\n",
    "    # get feature importance for fold k\n",
    "    dd = model.get_score(importance_type='weight')\n",
    "    df = pd.DataFrame({'feature':dd.keys(),f'importance_{fold}':dd.values()})\n",
    "    importances.append(df)\n",
    "\n",
    "    # infer oof fold k\n",
    "    oof_preds = model.predict(dvalid)\n",
    "    acc = amex_metric_mod(y_valid.values, oof_preds)\n",
    "    print('Kaggle Metric =',acc,'\\n')\n",
    "\n",
    "    # save oof\n",
    "    df = train.loc[valid_idx, ['customer_ID','target'] ].copy()\n",
    "    df['oof_pred'] = oof_preds\n",
    "    oof.append( df )\n",
    "\n",
    "    del dtrain, Xy_train, dd, df\n",
    "    del X_valid, y_valid, dvalid, model\n",
    "    _ = gc.collect()\n",
    "\n",
    "print('#'*25)\n",
    "oof = pd.concat(oof,axis=0,ignore_index=True).set_index('customer_ID')\n",
    "acc = amex_metric_mod(oof.target.values, oof.oof_pred.values)\n",
    "print('OVERALL CV Kaggle Metric =',acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# clean ram\n",
    "del train\n",
    "_ = gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "oof_xgb = pd.read_parquet(TRAIN_PATH, columns=['customer_ID']).drop_duplicates()\n",
    "oof_xgb['customer_ID_hash'] = oof_xgb['customer_ID'].apply(lambda x: int(x[-16:],16) ).astype('int64')\n",
    "oof_xgb = oof_xgb.set_index('customer_ID_hash')\n",
    "oof_xgb = oof_xgb.merge(oof, left_index=True, right_index=True)\n",
    "oof_xgb = oof_xgb.sort_index().reset_index(drop=True)\n",
    "oof_xgb.to_csv(f'oof_xgb_v{VER}.csv',index=False)\n",
    "oof_xgb.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot oof predictions\n",
    "plt.hist(oof_xgb.oof_pred.values, bins=100)\n",
    "plt.title('OOF Predictions')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# clear vram, ram for inference below\n",
    "del oof_xgb, oof\n",
    "_ = gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = importances[0].copy()\n",
    "for k in range(1,FOLDS): df = df.merge(importances[k], on='feature', how='left')\n",
    "df['importance'] = df.iloc[:,1:].mean(axis=1)\n",
    "df = df.sort_values('importance',ascending=False)\n",
    "df.to_csv(f'xgb_feature_importance_v{VER}.csv',index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NUM_FEATURES = 20\n",
    "plt.figure(figsize=(10,5*NUM_FEATURES//10))\n",
    "plt.barh(np.arange(NUM_FEATURES,0,-1), df.importance.values[:NUM_FEATURES])\n",
    "plt.yticks(np.arange(NUM_FEATURES,0,-1), df.feature.values[:NUM_FEATURES])\n",
    "plt.title(f'XGB Feature Importance - Top {NUM_FEATURES}')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculate size of each separate test part\n",
    "def get_rows(customers, test, NUM_PARTS = 4, verbose = ''):\n",
    "    chunk = len(customers)//NUM_PARTS\n",
    "    if verbose != '':\n",
    "        print(f'We will process {verbose} data as {NUM_PARTS} separate parts.')\n",
    "        print(f'There will be {chunk} customers in each part (except the last part).')\n",
    "        print('Below are number of rows in each part:')\n",
    "    rows = []\n",
    "\n",
    "    for k in range(NUM_PARTS):\n",
    "        if k==NUM_PARTS-1: cc = customers[k*chunk:]\n",
    "        else: cc = customers[k*chunk:(k+1)*chunk]\n",
    "        s = test.loc[test.customer_ID.isin(cc)].shape[0]\n",
    "        rows.append(s)\n",
    "    if verbose != '': print( rows )\n",
    "    return rows,chunk\n",
    "\n",
    "# compute size of 4 parts for test data\n",
    "NUM_PARTS = 4\n",
    "TEST_PATH = os.path.join(data, 'test.parquet')\n",
    "\n",
    "print(f'Reading test data...')\n",
    "test = read_file(path = TEST_PATH, usecols = ['customer_ID','S_2'])\n",
    "customers = test[['customer_ID']].drop_duplicates().sort_index().values.flatten()\n",
    "rows,num_cust = get_rows(customers, test[['customer_ID']], NUM_PARTS=NUM_PARTS, verbose='test')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# infer test data in parts\n",
    "skip_rows = 0\n",
    "skip_cust = 0\n",
    "test_preds = []\n",
    "\n",
    "for k in range(NUM_PARTS):\n",
    "    # read part of test data\n",
    "    print(f'\\nReading test data...')\n",
    "    test = read_file(path = TEST_PATH)\n",
    "    test = test.iloc[skip_rows:skip_rows+rows[k]]\n",
    "    skip_rows += rows[k]\n",
    "    print(f'=> Test part {k+1} has shape', test.shape )\n",
    "\n",
    "    # process and feature engineer part of test data\n",
    "    test = process_and_feature_engineer(test)\n",
    "    if k==NUM_PARTS-1: test = test.loc[customers[skip_cust:]]\n",
    "    else: test = test.loc[customers[skip_cust:skip_cust+num_cust]]\n",
    "    skip_cust += num_cust\n",
    "\n",
    "    # test data for xgb\n",
    "    X_test = test[FEATURES]\n",
    "    dtest = xgb.DMatrix(data=X_test)\n",
    "    test = test[['P_2_mean']] # reduce memory\n",
    "    del X_test\n",
    "    gc.collect()\n",
    "\n",
    "    # infer xgb models on test data\n",
    "    model = xgb.Booster()\n",
    "    model.load_model(f'XGB_v{VER}_fold0.xgb')\n",
    "    preds = model.predict(dtest)\n",
    "    for f in range(1, FOLDS):\n",
    "        model.load_model(f'XGB_v{VER}_fold{f}.xgb')\n",
    "        preds += model.predict(dtest)\n",
    "    preds /= FOLDS\n",
    "    test_preds.append(preds)\n",
    "\n",
    "    # clean memory\n",
    "    del dtest, model\n",
    "    _ = gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# write submission file\n",
    "test_preds = np.concatenate(test_preds)\n",
    "test = cudf.DataFrame(index=customers,data={'prediction':test_preds})\n",
    "sub = cudf.read_csv(os.path.join(data, 'sample_submission.csv')[['customer_ID']])\n",
    "sub['customer_ID_hash'] = sub['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n",
    "sub = sub.set_index('customer_ID_hash')\n",
    "sub = sub.merge(test[['prediction']], left_index=True, right_index=True, how='left')\n",
    "sub = sub.reset_index(drop=True)\n",
    "\n",
    "# display predictions\n",
    "sub.to_csv(f'submission_xgb_v{VER}.csv',index=False)\n",
    "print('Submission file shape is', sub.shape )\n",
    "sub.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot predictions\n",
    "plt.hist(sub.to_pandas().prediction, bins=100)\n",
    "plt.title('Test Predictions')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}